{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce38d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4822c86",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df942ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avgOutPoint1(df):\n",
    "    #Change point 1 readings to Avg. of Point 2.\n",
    "    label_list=list(df['label'])\n",
    "    conc_list = list(df['AT400(CO2 %)'])\n",
    "    p=0\n",
    "    num=0\n",
    "    i=0\n",
    "    sampling2_avgcon = list()\n",
    "    while i<len(label_list):\n",
    "        if label_list[i]==2:\n",
    "            p=p+conc_list[i]\n",
    "            num=num+1\n",
    "            i=i+1\n",
    "        else:\n",
    "            if p ==0 and num==0:\n",
    "                i=i+1\n",
    "                pass\n",
    "            else:\n",
    "                sampling2_avgcon.append(p/num)\n",
    "                i =i+1\n",
    "                num=0\n",
    "                p=0\n",
    "\n",
    "    i=0\n",
    "    k=-1\n",
    "    while i<(len(label_list)-1):\n",
    "        if label_list[i]==1:\n",
    "            conc_list[i]=sampling2_avgcon[k]\n",
    "            i=i+1\n",
    "        else:\n",
    "            if label_list[i+1]==1 :\n",
    "                k=k+1\n",
    "                i=i+1\n",
    "            else:\n",
    "                i=i+1\n",
    "    #set new AT400\n",
    "    df['AT400(CO2 %)']=pd.Series(data=conc_list,index=df.index)\n",
    "    return df\n",
    "\n",
    "def columnSeparator(df):\n",
    "    # linear interpolation to fill missing values\n",
    "    for i in range(1,7): # 6 sampling points\n",
    "        new_con =list()\n",
    "        j = 0\n",
    "        while j<df.shape[0]:\n",
    "            if df.iloc[j]['label']==i:\n",
    "                new_con.append( df.iloc[j]['AT400(CO2 %)'])\n",
    "                j = j+1\n",
    "            else:\n",
    "                new_con.append(np.nan)\n",
    "                j = j+1\n",
    "        df[str(i)+\"_sampling\"]=pd.Series(data=new_con,index=df.index)\n",
    "        df[str(i)+\"_sampling\"]=df[str(i)+\"_sampling\"].interpolate(method=\"linear\")\n",
    "    # Notice the df will contain NaN for the first few rows for each sampling point column\n",
    "    return df\n",
    "\n",
    "def getSampleSet(df_list, callback, train_feature_list, label_list):\n",
    "    conc_label_list=[]\n",
    "    for i in range(len(df_list)):\n",
    "        # The 1st element in label list is label\n",
    "        nset = df_list[i].shape[0]-callback\n",
    "        print(\"Number of set: \", nset)\n",
    "        conc_input=np.zeros((df_list[i].shape[0],6)) # Only the true reading is non-zero\n",
    "        conc_set = df_list[i][label_list[1:]].values # Excl. 1st column, label column\n",
    "        conc_set[np.isnan(conc_set)]=0\n",
    "        parameter_set = df_list[i][train_feature_list].values\n",
    "        for k in range(conc_input.shape[0]): # Only the true reading is non-zero, default for onehot\n",
    "            conc_input[k][int(df_list[i]['label'][k])-1]=1        \n",
    "        conc_input[np.isnan(conc_input)]=0\n",
    "        parameter_set=np.hstack((parameter_set,conc_input))\n",
    "        sample_set=np.zeros((nset,callback+1,parameter_set.shape[1]))\n",
    "        label_set=np.zeros((nset,1,conc_set.shape[1]))\n",
    "        for j in range(nset):\n",
    "            sample_set[j]=parameter_set[0+j:callback+j+1]\n",
    "            label_set[j]=conc_set[callback+j]\n",
    "        \n",
    "        if (i==0):\n",
    "            total_sample_set=np.zeros((0,callback+1,parameter_set.shape[1]))\n",
    "            total_label_set=np.zeros((0,1,conc_set.shape[1]))\n",
    "        conc_label_list.append(label_set)\n",
    "        total_sample_set = np.vstack([total_sample_set,sample_set])\n",
    "        total_label_set = np.vstack([total_label_set,label_set])\n",
    "        \n",
    "    return total_sample_set, total_label_set, conc_label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44898940",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_idx=6\n",
      "test file is ./data/140207_1.xlsx\n",
      "Origin input DF feature num(excl time, incl. label):  91\n",
      "concentration max, min:  [12.03671837] [0.]\n",
      "train list:  7\n"
     ]
    }
   ],
   "source": [
    "data_root_path = './data'\n",
    "data_path_list = glob.glob(os.path.join(data_root_path, \"1*.xlsx\"))\n",
    "list_of_raw_df = []\n",
    "test_set = '140207_1'\n",
    "df_list = []\n",
    "for i, path in enumerate(data_path_list):        \n",
    "    xls = pd.ExcelFile(path)\n",
    "    data_df = pd.read_excel(xls, sheet_name=0, index_col=0, header=[0,1])\n",
    "    data_df.columns = data_df.columns.map(''.join)\n",
    "    data_df=data_df.rename_axis('time').reset_index()\n",
    "    tmp_name = list(data_df.columns)\n",
    "    tmp_name[-1] = 'label'\n",
    "    data_df.columns = tmp_name\n",
    "    df_list.append(data_df)\n",
    "    if test_set in path:\n",
    "        test_idx = i\n",
    "print(\"test_idx={}\".format(test_idx))\n",
    "print(\"test file is {}\".format(data_path_list[test_idx]))\n",
    "\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i] = df_list[i].set_index(\"time\")\n",
    "    df_list[i] = avgOutPoint1(df_list[i])\n",
    "    if i == 0:\n",
    "        origin_feature_num = df_list[i].shape[1]\n",
    "        print(\"Origin input DF feature num(excl time, incl. label): \", origin_feature_num)\n",
    "        \n",
    "test_df_list = list(pd.Series(df_list)[[test_idx]]) # The 140207_1 is selected as the testset\n",
    "train_df_list = list(pd.Series(df_list)[list(set(range(0,len(df_list),1))-set([test_idx]))])\n",
    "\n",
    "for i in range(len(train_df_list)):\n",
    "    if (i==0):\n",
    "        tmp_full_values=train_df_list[i].values\n",
    "        tmp_conc_values=train_df_list[i]['AT400(CO2 %)'].values\n",
    "    else:\n",
    "        tmp_full_values=np.concatenate((tmp_full_values, train_df_list[i].values), axis=0)\n",
    "        tmp_conc_values=np.concatenate((tmp_conc_values, train_df_list[i]['AT400(CO2 %)'].values), axis=0)\n",
    "tmp_full_values = tmp_full_values[:,:-1] # excl. label column as well\n",
    "general_scaler = MinMaxScaler()\n",
    "conc_scaler = MinMaxScaler()\n",
    "general_scaler.fit(tmp_full_values)\n",
    "conc_scaler.fit(tmp_conc_values.reshape(-1,1))\n",
    "# general_max, general_min = general_scaler.data_max_, general_scaler.data_min_\n",
    "conc_max, conc_min = conc_scaler.data_max_, conc_scaler.data_min_\n",
    "print(\"concentration max, min: \", conc_max, conc_min)\n",
    "\n",
    "# Process into + ['label', '1_sampling', '2_sampling', '3_sampling', '4_sampling', '5_sampling', '6_sampling']\n",
    "for i in range(len(df_list)):\n",
    "    df_list[i].iloc[:,:-1] = general_scaler.transform(df_list[i].iloc[:,:-1].values)\n",
    "    df_list[i] = columnSeparator(df_list[i])\n",
    "    df_list[i]=df_list[i].fillna(0)\n",
    "\n",
    "# Separate df_list into Train & Test df lists\n",
    "test_df_list = list(pd.Series(df_list)[[test_idx]]) # The 140207_1 is selected as the testset\n",
    "train_df_list = list(pd.Series(df_list)[list(set(range(0,len(df_list),1))-set([test_idx]))])\n",
    "\n",
    "print(\"train list: \", len(train_df_list))\n",
    "label_list=['label', '1_sampling', '2_sampling', '3_sampling', '4_sampling', '5_sampling', '6_sampling']\n",
    "train_feature_list = list(np.sort(list(set(df_list[0].columns)-set(label_list))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c566163",
   "metadata": {},
   "source": [
    "# Save processed data into csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17a3017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed_data/train_data_0.csv\n",
      "Saved processed_data/train_data_1.csv\n",
      "Saved processed_data/train_data_2.csv\n",
      "Saved processed_data/train_data_3.csv\n",
      "Saved processed_data/train_data_4.csv\n",
      "Saved processed_data/train_data_5.csv\n",
      "Saved processed_data/train_data_6.csv\n",
      "Saved processed_data/test_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Save training DataFrames\n",
    "root_path = 'processed_data'\n",
    "\n",
    "if not os.path.exists(root_path):\n",
    "    os.makedirs(root_path)\n",
    "for i, df in enumerate(train_df_list):\n",
    "    filename = f'{root_path}/train_data_{i}.csv'\n",
    "    df.reset_index().to_csv(filename, index=False)\n",
    "    print(f\"Saved {filename}\")\n",
    "\n",
    "count = 0\n",
    "# Save test DataFrame (since test_df_list contains only one DataFrame)\n",
    "test_df_list[0].reset_index().to_csv(f'{root_path}/test_data.csv', index=False)\n",
    "print(f\"Saved {root_path}/test_data.csv\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c19e8b2",
   "metadata": {},
   "source": [
    "# Save JSON file for train and test loader with specific window size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4e5dbd80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed_data/train_data_windows_50.json with 430 windows\n",
      "Saved processed_data/test_data_windows_50.json with 68 windows\n",
      "Saved processed_data/dataset_metadata_50.json\n",
      "\n",
      "Dataset Summary:\n",
      "Window size: 50\n",
      "Total training windows: 430\n",
      "Total test windows: 68\n",
      "Training files: 7\n",
      "Test files: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_data = []\n",
    "test_data = []\n",
    "window_size = 50\n",
    "count = 0\n",
    "for i, df in enumerate(train_df_list):\n",
    "    num_data = df.shape[0]\n",
    "    num_windows = num_data - window_size\n",
    "    for j in range(num_windows):\n",
    "        train_data.append({})\n",
    "        train_data[count]['filename'] = f'{root_path}/train_data_{i}.csv'\n",
    "        train_data[count]['start_idx'] = j   \n",
    "        count += 1 \n",
    "\n",
    "count = 0\n",
    "num_data = test_df_list[0].shape[0]\n",
    "num_windows = num_data - window_size\n",
    "for j in range(num_windows):\n",
    "    test_data.append({})\n",
    "    test_data[count]['filename'] = f'{root_path}/test_data.csv'\n",
    "    test_data[count]['start_idx'] = j   \n",
    "    count += 1 \n",
    "\n",
    "# Save train_data and test_data as JSON files\n",
    "train_json_filename = f'{root_path}/train_data_windows_{window_size}.json'\n",
    "test_json_filename = f'{root_path}/test_data_windows_{window_size}.json'\n",
    "\n",
    "# Save training data windows\n",
    "with open(train_json_filename, 'w') as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "print(f\"Saved {train_json_filename} with {len(train_data)} windows\")\n",
    "\n",
    "# Save test data windows\n",
    "with open(test_json_filename, 'w') as f:\n",
    "    json.dump(test_data, f, indent=2)\n",
    "print(f\"Saved {test_json_filename} with {len(test_data)} windows\")\n",
    "\n",
    "# Also save metadata about the window configuration\n",
    "metadata = {\n",
    "    'window_size': window_size,\n",
    "    'num_train_files': len(train_df_list),\n",
    "    'num_test_files': len(test_df_list),\n",
    "    'num_train_windows': len(train_data),\n",
    "    'num_test_windows': len(test_data),\n",
    "    'train_file_shapes': [df.shape for df in train_df_list],\n",
    "    'test_file_shapes': [df.shape for df in test_df_list],\n",
    "    'label_list': label_list,\n",
    "    'train_feature_list': train_feature_list\n",
    "}\n",
    "\n",
    "metadata_filename = f'{root_path}/dataset_metadata_{window_size}.json'\n",
    "with open(metadata_filename, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "print(f\"Saved {metadata_filename}\")\n",
    "\n",
    "print(f\"\\nDataset Summary:\")\n",
    "print(f\"Window size: {window_size}\")\n",
    "print(f\"Total training windows: {len(train_data)}\")\n",
    "print(f\"Total test windows: {len(test_data)}\")\n",
    "print(f\"Training files: {len(train_df_list)}\")\n",
    "print(f\"Test files: {len(test_df_list)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb1e7c74",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
